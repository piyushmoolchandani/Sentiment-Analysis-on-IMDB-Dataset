{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Movie Review Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import dependies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from random import choice\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "> Try to get all train reviews in one train_reviews file and all text reviews in one test_reviews file (similarly for labels in train_labels, test_labes). This makes easier to gather set of all the words in all reviews and one hot encoded data.\n",
    "\n",
    "While moving the data into new files they are also stored in a randomized way to remove any kind of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b0264616982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_reviews.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_reviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_labels.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\train\\\\pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone')\n",
    "with open('train_reviews.txt', 'wb') as train_reviews:\n",
    "    with open('train_labels.txt', 'wb') as train_labels:\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\train\\\\pos')\n",
    "        pos = ['pos\\\\'+i for i in os.listdir()]\n",
    "        pos_index = 0\n",
    "        max_pos_index = len(pos)\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\train\\\\neg')\n",
    "        neg = ['neg\\\\'+i for i in os.listdir()]\n",
    "        neg_index = 0\n",
    "        max_neg_index = len(neg)\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\train')\n",
    "        for _ in range(max_pos_index + max_neg_index):\n",
    "            \n",
    "            if pos_index == max_pos_index:\n",
    "                pos_or_neg = 0\n",
    "                pos_complete = 1\n",
    "            elif neg_index == max_neg_index:\n",
    "                pos_or_neg = 1\n",
    "                neg_complete = 1\n",
    "            else:\n",
    "                pos_or_neg = choice([0, 1])\n",
    "                \n",
    "            if pos_or_neg:\n",
    "                with open(pos[pos_index], 'rb') as f:\n",
    "                    review = f.read() + '\\n'.encode()\n",
    "                    train_reviews.write(review)\n",
    "                    train_labels.write('1\\n'.encode())\n",
    "                pos_index += 1\n",
    "            else:\n",
    "                with open(neg[neg_index], 'rb') as f:\n",
    "                    review = f.read() + '\\n'.encode()\n",
    "                    train_reviews.write(review)\n",
    "                    train_labels.write('0\\n'.encode())\n",
    "                neg_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fb04955c68b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_reviews.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest_reviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_labels.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\test\\\\pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone')\n",
    "with open('test_reviews.txt', 'wb') as test_reviews:\n",
    "    with open('test_labels.txt', 'wb') as test_labels:\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\test\\\\pos')\n",
    "        pos = ['pos\\\\'+i for i in os.listdir()]\n",
    "        pos_index = 0\n",
    "        max_pos_index = len(pos)\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\test\\\\neg')\n",
    "        neg = ['neg\\\\'+i for i in os.listdir()]\n",
    "        neg_index = 0\n",
    "        max_neg_index = len(neg)\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone\\\\aclImdb\\\\test')\n",
    "        for _ in range(max_pos_index + max_neg_index):\n",
    "            \n",
    "            if pos_index == max_pos_index:\n",
    "                pos_or_neg = 0\n",
    "                pos_complete = 1\n",
    "            elif neg_index == max_neg_index:\n",
    "                pos_or_neg = 1\n",
    "                neg_complete = 1\n",
    "            else:\n",
    "                pos_or_neg = choice([0, 1])\n",
    "                \n",
    "            if pos_or_neg:\n",
    "                with open(pos[pos_index], 'rb') as f:\n",
    "                    review = f.read() + '\\n'.encode()\n",
    "                    test_reviews.write(review)\n",
    "                    test_labels.write('1\\n'.encode())\n",
    "                pos_index += 1\n",
    "            else:\n",
    "                with open(neg[neg_index], 'rb') as f:\n",
    "                    review = f.read() + '\\n'.encode()\n",
    "                    test_reviews.write(review)\n",
    "                    test_labels.write('0\\n'.encode())\n",
    "                neg_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a763a83d18f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Go to main directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone'"
     ]
    }
   ],
   "source": [
    "# Go to main directory\n",
    "os.chdir('C:\\\\Users\\\\Piyush\\\\3_DeepLearning\\\\Capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test data\n",
    "with open('train_reviews.txt', 'rb') as f:\n",
    "    reviews = f.read().decode().lower()\n",
    "with open('train_labels.txt', 'rb') as f:\n",
    "    labels = f.read().decode().lower()\n",
    "with open('test_reviews.txt', 'rb') as f:\n",
    "    test_reviews = f.read().decode().lower()\n",
    "with open('test_labels.txt', 'rb') as f:\n",
    "    test_labels = f.read().decode().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuation = '!\"#$%&()*+,./:;<=>?@[\\]^`{|}~'\n",
    "reviews = ''.join([i for i in reviews if i not in punctuation])\n",
    "test_reviews = ''.join([i for i in test_reviews if i not in punctuation])\n",
    "\n",
    "# Split the text into individual reviews.\n",
    "reviews = reviews.split('\\n')\n",
    "test_reviews = test_reviews.split('\\n')\n",
    "\n",
    "# Join all the reviews but with space as joining character in place of '\\n'.\n",
    "reviews_continuous = ' '.join(reviews)\n",
    "test_reviews_continuous = ' '.join(test_reviews)\n",
    "\n",
    "# Split text into individual words.\n",
    "reviews_words = reviews_continuous.split()\n",
    "test_reviews_words = test_reviews_continuous.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5833231\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of different words:  203683\n",
      "\n",
      "Examples of preprocessed data: \n",
      "[[33189, 323, 6, 3, 1062, 215, 8, 2023, 29, 1, 164, 59, 14, 46, 78, 6302, 41, 378, 122, 138, 14, 5113, 53, 4643, 153, 7, 1, 4114, 5999, 462, 69, 5, 254, 11, 33189, 64942, 2054, 6, 72, 2465, 5, 620, 70, 6, 5113, 1, 29480, 5, 1929, 10378, 1, 5567, 1432, 35, 67, 64, 206, 142, 63, 1155, 64943, 24601, 1, 38599, 4, 1, 213, 863, 30, 3081, 69, 4, 1, 5454, 9, 651, 2, 63, 1432, 50, 9, 203, 1, 395, 7, 61, 3, 1421, 3249, 740, 5, 3523, 185, 1, 378, 9, 1176, 13412, 29, 323, 3, 355, 367, 3489, 144, 137, 5, 9451, 28, 4, 124, 5113, 1421, 2581, 5, 33189, 323, 9, 509, 11, 105, 1530, 4, 53, 643, 102, 11, 33189, 323, 6, 226, 7814, 48, 3, 2271, 11, 8, 205]]\n",
      "[[9, 404, 2, 203, 10, 17, 234, 317, 100, 108, 31821, 5, 31, 3, 169, 348, 4, 1772, 637, 951, 11, 9, 13, 5654, 5, 64, 8, 84, 34, 48, 9, 651, 4, 10170, 10419, 27, 13, 60, 471, 5, 80, 215, 9, 13, 362, 10419, 250, 1, 110, 4, 3577, 17078, 52, 74, 2, 1583, 8226, 250, 1191, 8626, 15, 138, 11806, 1, 1977, 4, 3, 49, 17, 6, 11, 8, 67, 3174, 15, 251, 1301, 10, 28, 115, 597, 11, 1, 422, 761, 61, 13, 2951, 45, 13, 3143, 31, 2213, 294, 1, 88, 344, 4, 1, 17, 2, 68, 1582, 5, 1691, 294, 1, 335, 344, 134, 13078, 1, 761, 9, 21, 60, 203, 105, 373, 7, 1691, 18, 105, 376, 2321, 343, 14, 74, 258, 2742, 21, 5, 370, 247, 64, 94, 2471, 10, 17, 13, 82, 2, 9, 1409, 11, 22, 140, 64, 8, 157, 22, 1662]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Building a dictionary to map words with integers(in a sorted order of usage).\n",
    "count = Counter(reviews_words + test_reviews_words)\n",
    "sorted_count = sorted(count, key = count.get, reverse = True)\n",
    "sorted_count_dict = {word:i for i, word in enumerate(sorted_count, 1)}\n",
    "\n",
    "preprocessed_reviews = []\n",
    "preprocessed_test_reviews = []\n",
    "\n",
    "for review, test_review in zip(reviews, test_reviews):\n",
    "    preprocessed_reviews.append([sorted_count_dict[word] for word in review.split()])\n",
    "    preprocessed_test_reviews.append([sorted_count_dict[word] for word in test_review.split()])\n",
    "\n",
    "print('Total number of different words: ', len(sorted_count_dict))\n",
    "print('\\nExamples of preprocessed data: ', preprocessed_reviews[:1], preprocessed_test_reviews[:1], sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.split('\\n')\n",
    "test_labels = test_labels.split('\\n')\n",
    "\n",
    "labels = np.array([1 if label == '1' else 0 for label in labels])\n",
    "test_labels = np.array([1 if label == '1' else 0 for label in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove zero length reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_index = [i for i, review in enumerate(preprocessed_reviews) if len(review) != 0]\n",
    "non_zero_index_test = [i for i, review in enumerate(preprocessed_test_reviews) if len(review) != 0]\n",
    "\n",
    "preprocessed_reviews = [preprocessed_reviews[i] for i in non_zero_index]\n",
    "labels = np.array([labels[i] for i in non_zero_index])\n",
    "\n",
    "preprocessed_test_reviews = [preprocessed_test_reviews[i] for i in non_zero_index_test]\n",
    "test_labels = np.array([test_labels[i] for i in non_zero_index_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "1. We need to standardize review lengths. To find that length we take approximately the average length of all reviews as standard length.\n",
    "\n",
    "2. A function pad_it is defined, which returns an array that contains the padded data(with 0) or shortened data(depending on the length of data), of a standard size, that we'll pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_it(data, length):\n",
    "    \n",
    "    features = np.zeros((len(data), length), dtype=int)\n",
    "    for i, row in enumerate(data):\n",
    "        features[i, -len(row):] = np.array(row)[:length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard length to be used:  250\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "for i in preprocessed_reviews:\n",
    "    total_length += len(i)\n",
    "avg_length = total_length / len(preprocessed_reviews)\n",
    "review_length = math.ceil(avg_length / 50) * 50\n",
    "print(\"Standard length to be used: \", review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of padded data\n",
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [31200    38 87732    14   725 16374  3428    44    75    32]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [ 3806 24602   515    14     3  3403   167  8778 12375  1522]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   50     9    13   117    53   797   529    69   331     5]]\n"
     ]
    }
   ],
   "source": [
    "features = pad_it(preprocessed_reviews, review_length)\n",
    "features_test = pad_it(preprocessed_test_reviews, review_length)\n",
    "print('Example of padded data')\n",
    "print(features[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train data into train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASETS \n",
      "Train\t\t(20000, 250)\n",
      "Validation\t(5000, 250)\n",
      "Test\t\t(25000, 250)\n"
     ]
    }
   ],
   "source": [
    "validation_fraction = 0.2\n",
    "\n",
    "# Split the data\n",
    "train_length = len(features)*(1 - validation_fraction)\n",
    "train_length = int(train_length)\n",
    "train_X, train_y = features[:train_length], labels[:train_length]\n",
    "validation_X, validation_y = features[train_length:], labels[train_length:]\n",
    "test_X, test_y = features_test, test_labels\n",
    "\n",
    "print('DATASETS \\nTrain\\t\\t{0}\\nValidation\\t{1}\\nTest\\t\\t{2}'.format(train_X.shape, validation_X.shape, test_X.shape, sep = '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataloaders and Batches\n",
    "\n",
    "    TensorDataset and DataLoader from torch.utils.data can be used for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additional dependencies\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(validation_X), torch.from_numpy(validation_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n",
    "\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Benchmark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Naive Bayes based sentiment classifier is be used as benchmark model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_bench, train_y_bench = reviews[:25000], labels[:25000]\n",
    "test_X_bench, test_y_bench = test_reviews[:25000], test_y[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = vectorizer.fit_transform([i for i in train_X_bench])\n",
    "test_features = vectorizer.transform([i for i in test_X_bench])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_features[:len(train_y_bench)], [int(r) for r in train_y_bench])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes model accuracy: 81.624\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(np.equal(predictions, test_y_bench)) / len(predictions) * 100\n",
    "print(\"Multinomial Naive Bayes model accuracy: {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# Check for gpu\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if (gpu_available):\n",
    "    print(\"Training on GPU\")\n",
    "else:\n",
    "    print(\"Training on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Sentiment_Analysis_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, words_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):\n",
    "        super(Sentiment_Analysis_Network, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.words_size = words_size\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.words_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.n_layers, dropout = self.drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sig(out)\n",
    "        \n",
    "        out = out.view(batch_size, - 1)\n",
    "        out = out[:, -1]\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def initialize_hidden_layer(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (gpu_available):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment_Analysis_Network(\n",
      "  (embedding): Embedding(203684, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.4)\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "words_size = len(count) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = Sentiment_Analysis_Network(words_size, output_size, embedding_dim, hidden_dim, n_layers, 0.4)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hyperparameters and other components of neural networks choosen:\n",
    "    1. Loss : Binary Cross Entropy Loss\n",
    "    2. lr (learning rate) : kept small\n",
    "    3. epochs : kept moderate\n",
    "    4. clip (maximum gradient value) : in-between 1 to 5\n",
    "    5. optimizer : Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t Step: 100\t Loss: 0.619762\t Val Loss: 0.682943\n",
      "Epoch: 1\t Step: 200\t Loss: 0.635037\t Val Loss: 0.659172\n",
      "Epoch: 1\t Step: 300\t Loss: 0.658900\t Val Loss: 0.669557\n",
      "Epoch: 1\t Step: 400\t Loss: 0.440803\t Val Loss: 0.592388\n",
      "Epoch: 2\t Step: 500\t Loss: 0.562420\t Val Loss: 0.616970\n",
      "Epoch: 2\t Step: 600\t Loss: 0.618525\t Val Loss: 0.542491\n",
      "Epoch: 2\t Step: 700\t Loss: 0.464774\t Val Loss: 0.489873\n",
      "Epoch: 2\t Step: 800\t Loss: 0.673399\t Val Loss: 0.690893\n",
      "Epoch: 3\t Step: 900\t Loss: 0.696897\t Val Loss: 0.622874\n",
      "Epoch: 3\t Step: 1000\t Loss: 0.560164\t Val Loss: 0.501646\n",
      "Epoch: 3\t Step: 1100\t Loss: 0.454314\t Val Loss: 0.524874\n",
      "Epoch: 3\t Step: 1200\t Loss: 0.427792\t Val Loss: 0.652841\n",
      "Epoch: 4\t Step: 1300\t Loss: 0.357920\t Val Loss: 0.449767\n",
      "Epoch: 4\t Step: 1400\t Loss: 0.321248\t Val Loss: 0.465765\n",
      "Epoch: 4\t Step: 1500\t Loss: 0.349105\t Val Loss: 0.435598\n",
      "Epoch: 4\t Step: 1600\t Loss: 0.369470\t Val Loss: 0.432479\n",
      "Epoch: 5\t Step: 1700\t Loss: 0.127740\t Val Loss: 0.417446\n",
      "Epoch: 5\t Step: 1800\t Loss: 0.278519\t Val Loss: 0.422711\n",
      "Epoch: 5\t Step: 1900\t Loss: 0.197512\t Val Loss: 0.434423\n",
      "Epoch: 5\t Step: 2000\t Loss: 0.282327\t Val Loss: 0.521897\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "clip = 5\n",
    "print_every = 200\n",
    "counter = 0\n",
    "\n",
    "if gpu_available:\n",
    "    net.cuda()\n",
    "    \n",
    "net.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hiddens = net.initialize_hidden_layer(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        if (gpu_available):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            hiddens = tuple([each.data for each in hiddens])\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        output, hiddens = net(inputs, hiddens)\n",
    "\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if counter % print_every == 0:\n",
    "            val_hidden = net.initialize_hidden_layer(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "\n",
    "                if (gpu_available):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_hidden = net(inputs, val_hidden)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}\\t\".format(epoch + 1),\n",
    "                      \"Step: {}\\t\".format(counter),\n",
    "                      \"Loss: {:.6f}\\t\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Metric used for evaluation : Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.47336623013\n",
      "Test Accuracy:  0.81436\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "correct_count = 0\n",
    "\n",
    "hiddens = net.initialize_hidden_layer(batch_size)\n",
    "\n",
    "net.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    hiddens =  tuple([each.data for each in hiddens])\n",
    "    \n",
    "    if gpu_available:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        output, hiddens = net(inputs, hiddens)\n",
    "        \n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        predictions = torch.round(output.squeeze())\n",
    "        correct_tensor = predictions.eq(labels.float().view_as(predictions))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        correct_count += np.sum(correct)\n",
    "print(\"Test loss: \", np.mean(test_losses))\n",
    "test_accuracy = correct_count / len(test_loader.dataset)\n",
    "print(\"Test Accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
